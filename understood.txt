Adjunction, Hom inner product: this ontology comes from linear algebra. The idea is simple, the definition of a natural transformation, one definition of an adjunction can come from the definition of linear algebra, consider the adjunction under integral inner product of Int(fx, y) = Int(x, gy), where f and g are linear operators. Likewise, one can define adjunction as an inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY). This way, one can invent the definition of an inner product.

Categorification, replacing integers by sets: this adds dimensionality since you can put other objects as set, then reduce it to the case of integers by looking at grading of the set. You can then apply group theoretic techniques or techniques from number theory on the decategorified version of the set.

Complex numbers, example of  a ring: historically, this was done by Gauss to show unique factorisation. So the definition of complex numbers is to enable unique decomposition of integers as a product of prime integers.

Complex numbers, ordered pairs of numbers: once you consider these as ordered pairs of numbers, one can apply topological methods to it since these lie on two different axes, or to apply algebraic geometry. It also lends intuition to 1 and i being orthogonal vectors on the real-imaginary plane, and also motivates the complex axes. Methods from linear algebra in two dimensions, such as defining [a -b; b a] as the matrices that govern complex numbers. In fact it can be written as [cos theta -sin theta; sin theta cos theta] = cos theta * [1 0; 0 1] + sin theta * [0 -1; 1 0] and these matrices represent the real and imaginary unit vectors. Representation theory can then be applied. One can invent complex numbers by looking at the complex plane, and see how to represent it with matrices into one object.

Forgetful functor, right adjoint: recall that a forgetful functor forgets some structure. Consider the case of limits, where you have a big object with orthogonal projection maps. The forgetful functor forget structure and projects down onto the underlying set. Therefore, it is a right adjoint, and right adjoint preserves limits.

Group cohomology, characterisation of free group of n generators as wedge sum of circles as example of topological cohomology isomorphism on classifying spaces or delooping BG: the idea is to describe the shape of the tree group Z * Z * Z… generated by n letters can be compared to its interpretation in topology. Recall that to every group G, there is a topological space BG called the classifying space of the group, the idea of the classifying space is a functor, we have pi_1(BG) = G, and pi_k(BG) = 0, for k >= 2, the topological homology is isomorphic to the group cohomology so H^k(BG, Z), topological, is isomorphic to H^k(G, Z). Note that you can replace Z with other stuff. In the case of B(Z * … * Z) for n-letters, you can get it as the wedge sum of n circles, use Van-Kampen’s theorem and compute topological cohomology, so the group cohomology of this is H^k(Z * … Z) is Z for k = 0, Z^n for k = 1, vanishes otherwise. Therefore, one can invent the idea of group cohomology by seeing under classifying spaces, what is the topological cohomology of that space.

Group cohomology, invariants of a group representation: start with the fundamental group of a space. This ontology is from Hurewicz, the idea is to represent a space by its fundamental group since Hopf showed if all higher homotopy groups of a space vanish then cohomology is determined by the fundamental group, then we have pi_1(X) as the fundamental group or the number of pieces of paths. Group cohomology becomes by definition to be H_*(pi_1(X)) of the fundamental group is defined to be cohomology group of the space H_*(X) by abuse of notation, since cohomology can be defined for a group that is made into a module. This is how you can invent group cohomology of a space, when you try to see what groups capture a space fully.

Group cohomology, right derived functor: start by studying a group G by its group representations or G-module, a G -module is an abelian group M (note: different from using a finite set or permutations of set, see consideration of p-group fixed point theorem) together with a group action of G on M, every element of G acting as an automorphism of M, G is written as multiplicatively, M is additive, this is a module. Then consider the submodule of G-invariant elements (or fixed under G, orbit submodule). Now if N is a G-submodule of M, not true that M/N are found unlike the case of abelian group M is a set. The first group cohomology is invented to measure the difference H^1(G, N). Now, form the collection of all G-modules as a category, where morphisms are equivariant group homomorphisms that are group homomorphisms from f(gx) = g(f(x)). Send each module M to group of invariants (or orbits) M^G is a functor from category of G-modules to tech category of Ab of abelian groups, this is left exact, not right exact. Form right derived functor, these are the group cohomology H^n(G, M). This is how one events group cohomology, repeat the same idea for inventing orbits on a finite set, but make the G-module first, then send it to an abelian group of group actions to see how cohomology affects this.

Group modules, group action on abelian groups: a module is a generic version of a vector space, with multiplicative scalars and additive objects. To invent group modules or group representations, pick abelian groups as the additive objects for commutative sums, and group elements as multiplicative scalars. Consider group G acting on a set X, this gives X as a discrete union of orbits. To define G-modules, the idea is to have another abelian group, so we have a G-module is an abelian group M, together with a group action of G on M, M is additive abelian group, G is multiplicative (not abelian) group. Therefore, one can invent the idea of a group module is to at least have a group action on another abelian group.

Group, categories of one object with all isomorphisms: consider that compositions of morphisms are all morphisms, if you add the stipulation of invertibility, and only require to study the symmetries (invertibility) of one object, you get the definition of a group being a groupoid of one object, or alternatively a category of one object with all isomorphisms on that object. One can therefore invent the notion of a group, by knowing what is groupoid, and consider all isomorphisms on a single groupoid object.

Group, generators and relations: consider a finite free group of n-alphabets, this gives the finite symmetric group on n-objects, freely adjoining the alphabet. Since this is a free group, you want to be able compose and take away elements of the free group easily. To get general groups, add additional relators for product of group elements to be the identity of the group to further constrain the definition of a group, these are called the relations. In fact, generators and relations are sufficient to define any finite group, and are enough to define a group. These are called the presentation of the group. One can invent the notion of a group, by seeing how you can make a free group by composing group elements as freely as possible, then you impose some rules to give it more structure and uniqueness.

Group, group actions and representations: groups can be represented by their representations, from the group itself to the general linear group, you want compositions of linear representations be linear representations (or linear transformations), likewise for the trivial representations, and the existence of inverse representations. This also motivates the study of invariants of these matrices, therefore the character of representations is defined by the trace of these linear maps. One can invent the notion of a group, by simply looking at compositions of matrix actions, and see what representations, when multiplied, get back the same trivial representations.

Group, necklaces on a finite set: this ontology gives a very nice fixed point theorem, let group G be a group acting on a finite set X, if cardinality of the finite set X is not divisible by of the the order p of the group G, then there is a fixed point in set X for the group. Proof intuition, is to consider necklaces, we can look at the prime decomposition elements of the finite set X, if there is remainder, these cannot be part of cyclic action of subgroups of the group G, so these are fixed points. Formal statement: Let G be a finite p-group, and let finite set X be acted on by the action of the finite group G, the set of actions being X^G, we note that the cardinality of this set of actions |X^G| is of cardinality X mod p, in particular, if the cardinality of X is not divisible by order p, then there is a fixed point in X^G. Proof, the finite set X is the disjoint union of all orbits acted on by G (these look like distinct necklaces) all looking like G/H, in particular, if we have a case where it is not divisible by order p, then there exists a fixed point.

Group, permutation of roots: Galois invented this ontology, you want to permute the roots of a polynomial equation and compose these permutations. With this ontology, and consider the trivial permutation, and compositions of permutations to be permutations, one arrives at the definition of the group. Also, which permutation when composed with a given permutation gives the trivial permutation? This must be the inverse permutation. Further, one can invent how these permutations of roots work. Given a polynomial equation, you can get the roots of the polynomial equations, that are solutions to algebraic equations. You want to permute the roots of that solve these algebraic equations, to see if you can admit a solution by radicals.

Group, symmetries of object: Cayley invented this ontology, he invented Cayley's theorem, groups are isomorphic to permutation subgroups of the symmetric group. The object remains invariant, so you can compose symmetries to get back the original object, and you can consider the what to compose to the any symmetry transformation to get back the trivial symmetry of doing nothing. This ontology is obtained when one consider that there is a bijection between left action on group elements, to permutations of group elements. To prove Cayley's theorem, consider the mapping a group element x to the left action on the group element gx, since the composition of group elements is closed by preserving the symmetries of an object, this must be a bijection on the set of group elements. Since this is a bijection on the set of group elements, the set of group elements is in bijection to the permutation of the group elements, and is hence isomorphic to a permutation subgroup of the symmetric group. The symmetric group is also a automorphism group on sets. One can invent the notion of a group by considering all symmetries of an object.

Ideal, generalisation of even numbers as a ring through decomposition: say you want to decompose a ring like Z itself into unique factorisation, what are the conditions for decomposing Z to sets with prime factor (2), sets with prime factor (3) etc etera while still preserving uniqueness, the idea is to have ra, where r is in the ring, and a is in the ideal required, so start with basic element which is prime p in the ideal I, then consider scalar multiples of these basic elements to be a ring. Further, you want it to have the ring or module structure over the ring. So a ideal is a submodule of the ring, not just a set. For non-commutative ring, scalar multiples by the left give rise to left ideals, and is a left submodule of the full ring R. Therefore, one can invent the notion of an ideal by trying to see what sort of modules are factorised out of a full ring through decomposition.

Left adjoint, freest construction for representable: let G be a functor from category D to category C, you want to be able to build some sort of free construction on functor G acting backwards, then forget some more structure for a representable. This mimics the free construction of a group, then imposing additional relations to get whichever group you want. Also, this corresponds to the idea that a group presentation can be determined by its generators, then its relations. This also corresponds to a exact sequence or resolutions.

Left adjoint, preservation of free construction: consider the basic case of a free group, a left adjoint functor is something that acts on all of the alphabets / elements sentences in the free group and still preserves the sentence in the new category. The act of freely forming sentences is a form of colimit, see for example, abab being a + b + a + b (need not be abelian), we have F(a) + F(b) + F(a) + F(b). Therefore, the free functor, which is left adjoint (note, inner Hom product means I can act on FX to Y as a hom, and it is preserved, F is left adjoint. Consider the inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY), we have F is left adjoint to G, G is right adjoint F.

Number field, repeated additions, subtractions, multiplications and divisions generated from a number: the idea is to have a number as a generator, and see what other numbers an you get out of it by adding, subtracting, multiplying and dividing. Turns out these operations are already encoded in the rationals, so you can extend the rationals by this generator, so the definition of a number field is a finite extension of the field of rational numbers. So, one can invent the notion of a number field by seeing how one can freely add, subtract, multiply and divide if you add an additional generator.

Number ring, linear combinations of elements of a number field: the idea is to think of polynomials as linear combinations of a number in a number ring, based on a number field. If you can form the algebraic integer as a root fo a polynomial, it will satisfy the properties of a ring (since the component of a linear combination, or a root of a polynomial equation), therefore one can define a number ring to be the algebraic integers of a number field, where algebraic integers satisfy a polynomial equations, whose scalars multiples (using the analogy from linear algebra) come from a number field.

Orbit, generalisation of transitivity: say we have a transitive equivalence relation, if x is related to y, and y is related to z, then x is related to z. Instead, we can say a group is transitive on select group actions so it is transitive under group action g, but not f for example. For example, gx = ggy = gggz and these are transitive for this orbit for a left action g. Therefore, one can invent the notion of orbits as the elements in x that exhibit transitivity under left action g of group G. A group is therefore transitive if it has only one orbit. This way, one can invent multi dimensional or case by case transitivity.

Probability, normalised total space admitting integrals: the idea is you can start with measure theory, then use the formula for independent distributions as some sort of homomorphism P(XY) = P(X) P(Y). This condition of independence gives rise to what you can do with probability theory. Note that there is some sort higher order morphism thing going on since X and Y are measurable functions from the sample space to the real line. Then, to reinvent the definition of probability, simply pick P(sample space) = 1. Then you can invent the idea of a probability measure to be the normalised total space.

Random variable, kernel function of integral for probability: start with the idea that a probability is the Lebesgue integral of a probability density function. This comes from intuition of space as a dartboard, take a portion of an area where the total area is 1. Then, we have the case where we need to define a probability density function to be the inverse of a random variable mapping from the reals (our distribution) to the sample space. This is how one reinvents the definition of a random variable to be a measurable function from the sample space to the reals.

Representation, non-commutative rings: idea is to try and reduce algebraic structures into linear algebra, represent elements using linear transformations. Describe it with matrices, and matrix rings are not commutative, so representation theory reduces to the case of studying non-commutative rings. The representations of rings are called modules.

Right adjoint, underlying structure is preserved: consider the case of a limit. Under an action of a right adjoint, we have the inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY), we have F is left adjoint to G, G is right adjoint F. The arrows in the original category C, we have the homomorphisms from X to GY being preserved, G is the right adjoint functor, so we still keep the GY arrows)

Ring, generalising integer arithmetic: idea is to try and prove Fermat's last theorem. Initial idea is to adjoint root of -3 and consider a + b\root(-3). These form a ring, then try to find unique factorisation to show the n = 3 case. In fact, consider numbers of the form a + bw + cw^2, w is root of unity of 1 have unique factorisation. Basically, you consider properties of integers in two separate parts, then see if they combine nicely to still satisfy properties of integers. The idea is x^p + y^p = z^p can be converted to a product of (x + wy) = z^p by factorisation, for p-th root of unity, these rings fail to admit unique factorisation, idea is to generalise integers Z(w_p), fix this my inventing ideals. So, one can invent ring by adjoining roots and stuff.

Ring, generalising number fields, linear algebra and linear combinations through number rings: Hilbert was the first to make this generalisation. Previously, there were number fields. Then, we consider the algebraic integers that satisfy some linear combination (called a polynomial) with coefficients in the number field. These form a number ring. Hilbert generalised this further. But based on history, one can define elements of a ring to be those that can form a certain linear combination akin to a polynomial equation such that the polynomial can be factored as a product. One can invent the notion of a ring as elements that satisfy a linear combination relation.

Ring, linear algebra and factorisation: from the perspective that the a ring consists of elements that can form a linear combination akin to a polynomial, basically a ring are elements that enable linear combinations to be expressed as products of roots via factorisation, so one has a easy way to not just consider some inner product of coefficients and the root itself, but simply just consider the product of roots, for easy composition. The hard part is the unique factorisation. One can invent the notion of a ring to be elements that can be formed into inner products of coefficients and ring elements that can be factored into products of (x - root).

Yoneda lemma, from the perspective of natural equivalences: the idea is to define natural transformations, or transformations that are twice removed from conjugation. The typical example is Y'' is naturally isomorphic to Y, where ' is taking the dual of the vector space. The Yoneda lemma can be considered as a central definition where Nat(h_a, F) for the natural transformations between hom-sets of h_a and the functor F, is the double dual of the Hom functor Hom(Hom(A, -),F) from a fixed A (covariant means from A to stuff, contravariant reverses the arrow, so you can have Hom(G, Hom(-, A))) to be defined as natural transformations Nat(h^A, G). Then we have F as a functor to be invariant under double conjugation of Hom, so F(A) is naturally isomorphic to Hom(Hom(A, -), F), and we can define this as the natural transformations from morphism h_A to F(A). Note that the original motivation for inventing category theory is say there is an isomorphism between the locally compact ableian group with its twice iterated character group, or there is a natural isomorphism between L -> T^2(L). This is Pontrajagin duality. One can therefore invent the Yoneda lemma as the natural isomorphism between a covariant/contravariant functor of an object, and the double Hom into/out of a fixed object, with the conjugating functor being the Hom-functor.