$(-1)$-groupoid, every two elements of X is equal: the idea is to start with an $(-2)$-groupoid or a contractible type where all points are equal to the point in $x$, this gives the type definition $\Pi_{x in X} \Pi_{y in X} (y = x)$, dependent sum forces all points to be equal to x as we vary (recall dependent product as projections) y. Now, instead we change the dependent sum to dependent product $\Pi_{x in X} \Pi_{y in X} (y = x)$ means every two elements of X is equal to each other. Therefore, this has the structure of a $(-1)$-groupoid, with negative thinking forcing equality on pairs of elements and it is valued in $\{T, F\}$ as opposed to a single {true} in a $(-2)$-groupoid. One invents a $(-1)$-groupoid by consdiering this structure.

$(-1)$-groupoids, classification of groupoids: the ontology is this, one can consider the cases of $n$-groupoids and what do they correspond to, to reinvent them, one will need to consider negative dimensions of groupoids. Therefore, one can start with a $0$-groupoids which classifies containment in a set, $(-1)$-groupoids that are classified up to the empty groupoid and the terminal groupoid with one morphism or partial/total order from the empty groupoid to the terminal groupoid.

$(-2)$-groupoids, classification of groupoids: the ontology is this, one can consider the cases of n-groupoids and what do they correspond to, to reinvent them, one will need to consider negative dimensions of groupoids. Therefore, one can start with a $0$-groupoids which classifies containment in a set, $(-1)$-groupoids that are classified up to the empty groupoid and the terminal groupoid with one morphism or partial/total order from the empty groupoid to the terminal groupoid, and $(-2)$-groupoids are correspondingly invented for the classification of groupoids with no morphisms or the trivial groupoid.

$(-2)$-groupoids, there exists an $x$ in $X$ such that every point $y$ in $X$ is equal to $x$ in $X$: this corresponds to the ontology that there is only the point and there is no classification so every path from $y$ to $x$ varies continuously and contracts the space $X$ to a point $x$, therefore, one can invent $(-2)$-groupoids by considering the groupoid where this is always the case, and it only contracts to one point.

Abelian categories, unification of cohomology: idea is to invent a category that has all the lemmas you want, snake and five lemma. Then consider cohomology as a derived functor on a suitable abelian category, therefore one can unify cohomology theory as a theory on abelian category and invent abelian categories. For example, group cohomology is the derived category of Ab, sheaf cohomology is the derived category on the sheaves of abelian groups.

Adeles, enables Poisson summation: this ontology came from classical number theory, the idea is in algebraic number theory, you want to embed a number field into the cartesian product of completions at archimedian absolute values, example is to try with algebraic integers, then you can do Poisson summation which relates the Fourier series of predioic summation to the functions continuous transform. Chevalley and Weil invented adeles by having the number field itself embedded into the cartesian product of completions at all absolute values, not just the archimedian absolute values.

Adeles, number theory translated to topology: the idea of inventing adeles algebraically as the embedding of the cartesian product of formal completions at all absolute values allows ones to use statements of the topology in these to prove theorems. One example is the finiteness of the ideal class group and the Dirichlet unit theorem being equivalent to the quotient of ideles (units of the ring of adeles) to be compact or discrete. Therefore, one can invent adeles as something with the topological properties needed to prove stuff in number theory.

Adjoining roots, Fermat's last theorem: the idea is to decompose the sum of $x^n$ and $y^n$ into the product of $x - root of {-1}^{2i + 1}y$, Then one can take the Z[root of -1] to do unique factorisation. So looking at Fermat's last theorem, one can invent the idea of adjoining roots.

Adjunction, Hom inner product: this ontology comes from linear algebra. The idea is simple, the definition of a natural transformation, one definition of an adjunction can come from the definition of linear algebra, consider the adjunction under integral inner product of I   1.1A   (fx, y) = Int(x, gy), where f and g are linear operators. Likewise, one can define adjunction as an inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY). This way, one can invent the definition of an inner product.

Adjunction, ceiling and floor of a thin category: work out this example, let a category be reals, objects are real numbers, morphisms are partial orders of reals. Consider subcategory of integers to reals, inclusion of integers to reals have left and right adjoint, the right adjoint is floor, the left adjoint is ceiling. For example, r <= n if ceil(r) <= n. Argue similarly for right adjoint, r <= n if r <= \floor(n), so the floor is right adjoint. One can generalise this example by the ontology of the hom inner product, and with r and n not necessarily real numbers but functors to identify left and right adjoints. And we change the partial order to that of a natural transformation when defining categorical adjoints.

Algebraic integer, definition of integers that can make finite extensions of a field: this is a lemma and ontology, the idea is to suppose an algebraic integer $a$ is in a finite extension of $L$, then we have $[L:K]$ is n less than infinity, then take $1, a, a^2, ..., a^n$, this is $n + 1$ elements of a n-dimensional vector space. Since this is of higher dimension, there must be a nontrivial linear relation $k_0 + k_1 a + ... k_n a^n$, so $a$ is algebraic. For the converse, suppose $p(x)$ is an irreducible polynomial in the field extension $K[x]$, take the quotient of the ideal $K[x]/(p)$ is a field. This is obviously a ring. This a ring, now we check existence of inverses, suppose $q(x)$ is in $K(x)/(p)$, $q(x)$ is nonzero, then $q$ and $p$ are coprime since $p(x)$ is irreducible. Therefore, $a(x)q(x) + b(x) p(x) = 1$ can be found by Euclidean algorithm, so $a(x)$ is inverse of $q(x)$ in $K(x)/((p(x)))$. Since $a$ is a root of $p(x)$ in $K[x]$, where $p$ is irreducible, so $K[x]/p(x)$ is a map to $L$ taking $x$ to $a$. One more part of this is this ontology means that sums, products, quotients and differences are algebraic as motivation, however this can be hard.

Algebraic integers, ideals for field extensions: the idea is you want to consider adjoining roots to a field, and you can consider what sort of integers form a ring, one invents the definition of algebraic integers as those that form the ring of integers of the field, so you can do number theory on this, define ideals sheaves. The idea is to make it possible to be used as ideals, it is the root fo some poly (root enables unique factorisation) of $p(x)$ is in the field extension $k[x]$.

Azumaya algebras, central simple algebras whose center is not a field: this ontology came from Azumaya to develop noncommutative stuff by Grothendieck, the idea is the have a field extension to a noncommutative thing first, so the notion of a central simple algebra is invented, where the center is the field, and the associative field algebra is simple like a field. One can invent Azumaya algebras by trying to define it such that the center is not a field, but a commutative local ring (unique maximal ideal). So one can invent the definition of the Azumaya algebra by defining the center to be a commutative ring $R$ with $R$-algebra $A$, and $A$ is separable (equivalent condition of simple).

Bonferroni correction, p-hacking correction: too many hypothesis tests means false positive on hypothesis hacking, reduce statistical power.  Works for uncorrelated hypotheses (direct categorical limit of with each study an orthogonal projection) by direct division of number of tests, significance / number of tests. Therefore, one can invent the Bonferroni correction as a way to correct for avoiding p-hacking by having too many hypothesis, under a crude method that the distributions are independent and identically distributed or the chief experiment is a direct limit of these distributions.

Borel-sigma field/algebra, smallest sigma algebra and open sets: say you want to do measure theory of spaces (typically locally Hausdorff) you want to find the smallest basis to do linear algebra with, so one invents the notion of a Borel sigma algebra, the smallest sigma algebra containing the open sets as some sort of base for the linear algebra of points.

Canonical isomorphism, uniqueness of isomorphism with universal properties: the idea is that you want universal properties of isomorphism, one can invent a canonical isomorphism as the case where it is the only isomorphism as a statement or property about the isomorphism that can be proved.

Categorification, replacing integers by sets: this adds dimensionality since you can put other objects as set, then reduce it to the case of integers by looking at grading of the set. You can then apply group theoretic techniques or techniques from number theory on the decategorified version of the set. Note that categorification came later.

Central simple algebras, noncommutative field extensions: the idea is say you want to study field extensions of noncommutative rings. But you are trapped, a commutative simple ring (ring with no two sided ideal beside the zero ideal and itself) is a field, so keep the simple condition and associate, with a field, a finite dimensional associative K-algebra with the name coming that the center is exactly K or the commutative elements of the central simple algebra as a field extension is the field itself. This means one can invent central simple algebras as a field extension to something that is noncommutative.

Characters, Dirichlet characters in number theory: Frobenius was inspired by characters in number theory or Dirichlet characters which is multiplicative, periodic over m, and nonzero if coprime. Dirichlet characters are characters for the cyclic group mod m, Frobenius generalized this to arbitrary groups, leading to representation theory.

Characters, class function invariant under conjugation: Frobenius had the original definition of finite groups with h_ijk be number of solutions of equations of equations abc = 1 for elements in each conjugacy class. Then, denote inverses of i and you have a_ijk = h_i’jk / card(C_i) with solving structural equations r_j r_k = a_ijk r_i, then define character of finite groups as some class function (imitating number theory) where char(g) = fr_qi / card(C_i) with some f to normalize the characters. Basically he was trying to count the “number of solutions” on a given conjugacy class. They want something constant on conjugacy classes or invariant under conjugation action, so these are class functions example [a -b; b a] trace is 2a, determinant is a^2 + b^2. Using Grassman analogy it is the square formed by the length of the hypothenuse.

Circuits, linear algebra on irreducible closed curves: ontology by Jordan, he invented a irreducible circuit as simple closed curves that is non self-intersecting and cannot be continuously transformed into a point, so a general circuit can be decomposed into irreducible circuits where c is the sum of m_i a_i, and it is reducible if it is equal to 0. The irreducible circuits a_i are independent if the sum of m_i a_i is nonzero (prevents nilpotency). It turns out one can invent circuits as linear algebra of irreducible closed curves to be used as a topological invariant.

Compactness, accumulation point for some infinite bounded subset: the idea you can infinitely large sets, and yet still have a point where all these points are too close to each other, this is the ontology by Frechet. So one can invent compactness to considr the example of an infinite bounded set, all clustered at one point eventually. This is related to compactness as preserving filtered colimits, so if have sequences of points (filtrations) being accumulated into a colimit, and this is used to define compact objects.

Complex numbers, example of  a ring: historically, this was done by Gauss to show unique factorisation. So the definition of complex numbers is to enable unique decomposition of integers as a product of prime integers.

Complex numbers, ordered pairs of numbers: once you consider these as ordered pairs of numbers, one can apply topological methods to it since these lie on two different axes, or to apply algebraic geometry. It also lends intuition to 1 and i being orthogonal vectors on the real-imaginary plane, and also motivates the complex axes. Methods from linear algebra in two dimensions, such as defining [a -b; b a] as the matrices that govern complex numbers. In fact it can be written as [cos theta -sin theta; sin theta cos theta] = cos theta * [1 0; 0 1] + sin theta * [0 -1; 1 0] and these matrices represent the real and imaginary unit vectors. Representation theory can then be applied. One can invent complex numbers by looking at the complex plane, and see how to represent it with matrices into one object.

Conjugacy classes, free loops under free homotopy: no base point loops are free loops, then define equivalence classes of free loops in the fundamental group

Conjugacy, reduction in dimension: one can reinvent the idea of conjugacy by considering that for a set under the action of a cyclic group of prime order p, one can consider only the product of the first p - 1 elements since the last element is just the inverse and contains redundant information (or actually it isn’t redundant, you are using the fact that the product of p elements is the identity). This idea is how one reinvents the need for conjugates, and is the reason for inventing the p-group fixed point theorem or Fermat’s little theorem.

Dedekind cuts, real numbers as representing objects of adjoint functors: Dedekind's idea is very categorical. Use poset structure of reals, define real as representing object of the set of rationals less than or equal to the real or a representing object for a representing adjoint functor. This is the Dedekind cut. General lesson, one can define an object representing some functor. Therefore one can indirectly invent Dedekind cuts, or corresponding representing objects of a category through considering real numbers.

Dedekind domains, unique factorisation into prime ideals as Fermat's last theorem: this ontology is due to Dedekind, the idea is to factorise multiplicatively preserving products by representing ring elements by ideal of multiples. One can invent the definition of a Dedekind domain to be either a field or to have the nice condition that every nonzero proper ideal factors into products of prime ideals or to factor into products (category theory applies here).

Degree of field extension, reducing fields to vector space from complex to reals: the idea is to try and describe the reals and the complex numbers as a vector space of two real numbers, therefore one can invent the notion of a degree of a field extension by seeing the reals as an a way to extend the complex numbers, so the degree of the field extension from reals to complex is of dimension 2, and now one can describe this a 2-dim vector space.

Delta ring, using intersections instead of relative complementation to prevent sets of infinite measure, one can construct a delta ring by instead of consider relative complementation, one uses countable intersections. This prevent sets of infinite measure with the following example where the countable union of intervals of [0,n] forms a delta ring and not a sigma ring since this is not bounded, it is not closed under relative complementation. Therefore, one can invent a delta ring as a stricter condition of a sigma algebra to prevent sets of infinite measure.

Determinant, solving equations: key example is Cramer’s rule by Cardan (Cramer did for arbitrary dimensions), and Seki’s use of determinants to solve equations (not systems though). Leibniz computed coefficient matrix determinant as an equation and not as a quantity which must be equal to 0. Laplace did similar stuff yet used the word resultant (gives results).

Determinant, volume: Grassmann popularized this. Lagrange was the first to do this for volume of tetrahedron. Called it determinant since it determines properties of quadratic form.

Direct quotient, preservations of categorical limits: this is an exact sequence, divisor represents the equivalence classes, the dividend is actually direct limit, direct quotient is a projection. Manipulate the equation to: categorical limit = divisor / projection * number of equivalence classes / projections. If a direct quotient can be done, it means the categorical limit is preserved, and there exists a forgetful functor to cardinality that exists. I first noticed this phenomenon when trying to understand the Bonferroni correction.

Equivalence of sets of same cardinality, infinite sets and one to one correspondence: the motivation of infinite sets is very tricky, this ontology is by Bolzano, he showed that even numbers are in one to one correspondence to the natural numbers. Cantor extended this argument to show that the reals cannot be put into one to one correspondence with the naturals using an argument with nested intervals. This also motivates one to invent the idea for equivalence of sets and motivates the Bernstein-Schroder theorem.

Exterior algebra, make geometry arithmetic: recall that Grassmann invented the Grassmannian which have subspaces of a space as points. Start with orthogonal units, adjoin them freely to form linear combinations or sum of a_i e_i. Try to define a volume of it, it fails since you should get negative volume to get multi linearity (in particular, two vectors agree if their oriented volume is 0)

Exterior algebra, quantum mechanics and noncommutative geometry: recall that points are maximal ideals can be studied as algebraic geometry (which is commutative in nature). But, quantum mechanics with points mean observables are commutative ring, so do not use points, use Grassmann numbers or points of the exterior algebras in the first place. Define fermions like this.

Exterior algebra, two sides of algebra: the idea is one can decompose the tensor product of V tensor V into the symmetric square of V, with the symmetric algebra, and the alternative square of V, with the exterior / alternating algebra. So V tensor V is isomorphic to the direct sum of the symmetric square of V and the alternating square of V. They are C[G]-submodules of V tensor V. One can then generalise to define the symmetric and exterior powers as subspaces of the k-th tensor power, not the second tenor powers. No longer decompose as direct sum, but there is a Schur-Weyl duality that gives a formula or this that I do not understand.

Exterior product, multiplicands in exterior: Grassmann came up with the name. Idea is x ^ y ^ z is 0 if x lies in (not the exterior of) the subspace spanned by y and z, or x is in the parallelogram of y and z, so the exterior product vanishes.

Field extension, building up to field by starting with smaller fields to get bigger fields: the idea is to prove things from a smaller field L_0 to build up to L as a field extension. So one even field extensions by trying to prove stuff on smaller fields.

Field of set, sigma algebra with finiteness conditions, basically one can invent the notion of a field of sets by stipulating complementation, closure finite unions or equivalently, finite intersections (using De Morgan's laws).

Forgetful functor, right adjoint: recall that a forgetful functor forgets some structure. Consider the case of limits, where you have a big object with orthogonal projection maps. The forgetful functor forget structure and projects down onto the underlying set. Therefore, it is a right adjoint, and right adjoint preserves limits.

Fundamental group, definition of connectivity, ontology was first invented by Poincare and emphasised by Voevodsky in his lectures. The idea is to consider the group of loops that contract to a point, if the fundamental group is trivial i.e. all contract to a single point, one can invent the definition of simple connectedness and the idea of a fundamental group. Therefore, one can invent the notion of the (zeroth) fundamental group as the deformation of loops to points motivated by trying to define simple connectedness.

Fundamental group, homotopy, since homotopy we preserve end points, end points are invariants and hence they must be acted on by a group. Therefore one can invent homotopy and the fundamental groups, so one can invent infinity groupoids with k-morphisms

Fundamental group, making sure homotopies are unchanged: the fundamental group is a group, since there is something that is unchanged, the homotopies of paths are made equivalent, and therefore deforming one loop to another continiously leaves the loop unchanged. Therefore, one can invent the idea of a fundamental group as a group when one realises one wants the path unchanged. Remember, if you want something unchanged under action (or concatenation of loops), then you should think of a group, in this case it is the fundamental group.

Global sections functor, representable functor of the affine line: the idea is to think about what functor should an affine line represent as an representing object. One can invent the global sections functor from a topological space X to the global sections of the space X and the sheaf of rings on X through the Yoneda lemma. Some other examples would be Picard schemes representing and defining a Picard functor, Hilbert schemes representing a functor associating a variety with a family of subvarieties.

Greatest common divisor, preserve information on scalars: consider gcd(ab, ac) = a gcd(b, c) holds because we are taking the greatest of something. This feels very much like adjunctions somehow, but I cannot pin it down.

Grothendieck group, symmetry of the relations of exact sequences: suppose we have short exact sequences and we want to see what exact sequences can we form out of them while preserving truth of these relations. Since we want to preserve something, we can consider its symmetries. Therefore, one can invent the notion of a Grothendieck group as a group of relations preserving some truth of exact relations, we present the generator, relations definitions here: we have a ring A, F be the be the category of left A modules, the Grothendieck group of the category F is an abelian group with generators for each left A module and relations [E] = [E’] + [E’’] associated with exact sequences such that 0 is to E is to E’ is to E’’ is to 0.

Group cohomology, characterisation of free group of n generators as wedge sum of circles as example of topological cohomology isomorphism on classifying spaces or delooping BG: the idea is to describe the shape of the tree group Z * Z * Z… generated by n letters can be compared to its interpretation in topology. Recall that to every group G, there is a topological space BG called the classifying space of the group, the idea of the classifying space is a functor, we have pi_1(BG) = G, and pi_k(BG) = 0, for k >= 2, the topological homology is isomorphic to the group cohomology so H^k(BG, Z), topological, is isomorphic to H^k(G, Z). Note that you can replace Z with other stuff. In the case of B(Z * … * Z) for n-letters, you can get it as the wedge sum of n circles, use Van-Kampen’s theorem and compute topological cohomology, so the group cohomology of this is H^k(Z * … Z) is Z for k = 0, Z^n for k = 1, vanishes otherwise. Therefore, one can invent the idea of group cohomology by seeing under classifying spaces, what is the topological cohomology of that space.

Group cohomology, invariants of a group representation: start with the fundamental group of a space. This ontology is from Hurewicz, the idea is to represent a space by its fundamental group since Hopf showed if all higher homotopy groups of a space vanish then cohomology is determined by the fundamental group, then we have pi_1(X) as the fundamental group or the number of pieces of paths. Group cohomology becomes by definition to be H_*(pi_1(X)) of the fundamental group is defined to be cohomology group of the space H_*(X) by abuse of notation, since cohomology can be defined for a group that is made into a module. This is how you can invent group cohomology of a space, when you try to see what groups capture a space fully.

Group cohomology, right derived functor: start by studying a group G by its group representations or G-module, a G -module is an abelian group M (note: different from using a finite set or permutations of set, see consideration of p-group fixed point theorem) together with a group action of G on M, every element of G acting as an automorphism of M, G is written as multiplicatively, M is additive, this is a module. Then consider the submodule of G-invariant elements (or fixed under G, orbit submodule). Now if N is a G-submodule of M, not true that M/N are found unlike the case of abelian group M is a set. The first group cohomology is invented to measure the difference H^1(G, N). Now, form the collection of all G-modules as a category, where morphisms are equivariant group homomorphisms that are group homomorphisms from f(gx) = g(f(x)). Send each module M to group of invariants (or orbits) M^G is a functor from category of G-modules to tech category of Ab of abelian groups, this is left exact, not right exact. Form right derived functor, these are the group cohomology H^n(G, M). This is how one events group cohomology, repeat the same idea for inventing orbits on a finite set, but make the G-module first, then send it to an abelian group of group actions to see how cohomology affects this.

Group modules, group action on abelian groups: a module is a generic version of a vector space, with multiplicative scalars and additive objects. To invent group modules or group representations, pick abelian groups as the additive objects for commutative sums, and group elements as multiplicative scalars. Consider group G acting on a set X, this gives X as a discrete union of orbits. To define G-modules, the idea is to have another abelian group, so we have a G-module is an abelian group M, together with a group action of G on M, M is additive abelian group, G is multiplicative (not abelian) group. Therefore, one can invent the idea of a group module is to at least have a group action on another abelian group.

Group, categories of one object with all isomorphisms: consider that compositions of morphisms are all morphisms, if you add the stipulation of invertibility, and only require to study the symmetries (invertibility) of one object, you get the definition of a group being a groupoid of one object, or alternatively a category of one object with all isomorphisms on that object. One can therefore invent the notion of a group, by knowing what is groupoid, and consider all isomorphisms on a single groupoid object.

Group, class group in algebraic number theory or deriving a third element that holds under associativity with sameness under left action: this ontology is given by Kronecker that is similar to Weber, the idea is to derive a third variable, assume commutative and associative laws hold and that for finite set a, b, c, ... then if ab not equal to ac then b not equal to c. Weber also defined a group of degree h, with the idea one can derive a third element for associativity to hold and sameness under left action in Weber's definition. This is the last definition before the formal axiomatisation of a group as a unital associative magma with an invertible operation.

Group, closure of symmetry for a compact object: in the ontology of a topological group or a Lie group, one wants to compose operations to get back where they started and to ensure the action of the symmetry means that object still exists and it is achievable. One can therefore reinvent the notion of a group by looking at what rules let you ensure the object remains kept and compact under the operation, and that it is achievable and perfectly reversible.

Group, closure under permutations or symmetric actions: the closure property is the most important property, since Galois and Cauchy define groups in terms of the closure properties alone. With permutations, all you need is closure in defining a group (and in fact is the key step of the Yoneda lemma to get your double Hom functor to be the same as natural isomorphisms to an object). Therefore, one can invent a group by forcing permutations to be closed.

Group, conjugate system of substitutions: Cauchy's first definition is that of substitutions and derived substitutions so we have a conjugate system of substitutions, or something like an action s is the same as taking action g, then doing s, then taking inverse of g similar to conjugate action gs(1/g).

Group, generators and relations: consider a finite free group of n-alphabets, this gives the finite symmetric group on n-objects, freely adjoining the alphabet. Since this is a free group, you want to be able compose and take away elements of the free group easily. To get general groups, add additional relators for product of group elements to be the identity of the group to further constrain the definition of a group, these are called the relations. In fact, generators and relations are sufficient to define any finite group, and are enough to define a group. These are called the presentation of the group. One can invent the notion of a group, by seeing how you can make a free group by composing group elements as freely as possible, then you impose some rules to give it more structure and uniqueness.

Group, group actions and representations: groups can be represented by their representations, from the group itself to the general linear group, you want compositions of linear representations be linear representations (or linear transformations), likewise for the trivial representations, and the existence of inverse representations. This also motivates the study of invariants of these matrices, therefore the character of representations is defined by the trace of these linear maps. One can invent the notion of a group, by simply looking at compositions of matrix actions, and see what representations, when multiplied, get back the same trivial representations.

Group, loss of space in homotopy: start with the idea of a fundamental group, or homotopy that covers some sort of area, if we forget this notion of homotopy covering areas, we get the fundamental notion of a group if we consider the end result of one base point. Therefore one can invent the notion of a group by saying the integral or area under homotopy is forgotten or vanishes.

Group, necklaces on a finite set: this ontology gives a very nice fixed point theorem, let group G be a group acting on a finite set X, if cardinality of the finite set X is not divisible by of the the order p of the group G, then there is a fixed point in set X for the group. Proof intuition, is to consider necklaces, we can look at the prime decomposition elements of the finite set X, if there is remainder, these cannot be part of cyclic action of subgroups of the group G, so these are fixed points. Formal statement: Let G be a finite p-group, and let finite set X be acted on by the action of the finite group G, the set of actions being X^G, we note that the cardinality of this set of actions |X^G| is of cardinality X mod p, in particular, if the cardinality of X is not divisible by order p, then there is a fixed point in X^G. Proof, the finite set X is the disjoint union of all orbits acted on by G (these look like distinct necklaces) all looking like G/H, in particular, if we have a case where it is not divisible by order p, then there exists a fixed point.

Group, permutation of roots: Galois invented this ontology, you want to permute the roots of a polynomial equation and compose these permutations. With this ontology, and consider the trivial permutation, and compositions of permutations to be permutations, one arrives at the definition of the group. Also, which permutation when composed with a given permutation gives the trivial permutation? This must be the inverse permutation. Further, one can invent how these permutations of roots work. Given a polynomial equation, you can get the roots of the polynomial equations, that are solutions to algebraic equations. You want to permute the roots of that solve these algebraic equations, to see if you can admit a solution by radicals.

Group, rules to classify objects in a space: start with an ontology of an Eilenberg-Maclane space or a BG space, where the space consists of other spaces or is a parameterisation of some objects. One can invent the notion of the group as the operations allowed to get from one space to another in this classifying space. Therefore, one can start with some classification of spaces with some rules that keeps the spaces of having the same property, the symmetry of keeping these spaces to have these same properties in the classifying space is therefore the fundamental group of the classifying space of the group.

Group, set of operations on the same object or a set of objects: this ontology is given by Burnside. It is quite close if you consider groupoids and generalises quite well. The idea is you want sameness in the object or set of objects to consider whatever operations you put on it such that this part of it remains the same. Note that Burnside and Cayley was interested in infinite groups as well. This is how one can invent the definition of a group (or even a groupoid) as set of operations on the same object.

Group, structure of an alphabet: say one starts with an alphabet and wants to be able to add letters and delete letters, in fact this is the nature of composition, since we use letters to denote composition, therefore one can reinvent the structure of a group as the correct conditions to let us compose morphisms and invertible morphisms for the morphisms to cancel. This ontology is why groups are important, they represent the best possible construction in composition of morphisms in algebra.

Group, symmetries of object: Cayley invented this ontology, he invented Cayley's theorem, groups are isomorphic to permutation subgroups of the symmetric group. The object remains invariant, so you can compose symmetries to get back the original object, and you can consider the what to compose to the any symmetry transformation to get back the trivial symmetry of doing nothing. This ontology is obtained when one consider that there is a bijection between left action on group elements, to permutations of group elements. To prove Cayley's theorem, consider the mapping a group element x to the left action on the group element gx, since the composition of group elements is closed by preserving the symmetries of an object, this must be a bijection on the set of group elements. Since this is a bijection on the set of group elements, the set of group elements is in bijection to the permutation of the group elements, and is hence isomorphic to a permutation subgroup of the symmetric group. The symmetric group is also a automorphism group on sets. One can invent the notion of a group by considering all symmetries of an object.

Groups, Cayley's idea of composition of its members: although Cayley's idea is now in modernity associated with categories, he wrote that a group is defined by the law of composition of its members, therefore one can invent a group by considering all compositions and what is the law associated with this.

Groups, method of determining computation of solvability of quintics by radicals: since the motivation for groups comes from Galois theory, one can invent the notion of groups by seeing when symmetry fails and something cannot be computed simply because it fails to preserve some symmetry. Note that Galois' original paper just computed with these and did not define it.

Homotopy type theory, symmetry of truths, define a point to be a -2-groupoid or a proposition, then consider symmetries of true statements, so types as points become infinitely groupoids considering from points to paths and so on.

Homotopy, equivalent under parameterization: I first saw this ontology in Homotopy Type Theory, the idea is the homotopy of a path and inverses covers areas and therefore is not exactly the identity, therefore one can invent homotopy to consider the entire area of paths and a path that covers no areas is equivalent, therefore this motivates homotopy as a higher dimensional operation.

Homotopy, filling space for paths between paths: if one consider family of paths with same end point then there are areas , so one can invent homotopy as a filling space of k+1 dimensions for k dimensional paths, inventing the notion of a k-morphism.

Ideal, (0,1)-sheaf on posite: one starts with a posite or a posetequipped with a Grothendieck topology or a category equipped with a coverage, then one can change the partial relation of a posite to work on a (0,1)-sheaf by making it valued in true values where true values means whether it exists for an ideal. For example, start with the definition that a posite has relation called a coverage of a poset where: (1) if u is related to V, and v is in V then there is a map from v to u; and (2) if u is related to v and there is a map from u' to u, then u' is related to V' such that for every v' in V', there is a v such that v' maps to v. One can reinvent ideals by giving the partial order itself as the relation, if u MAPS to V, and v is in V then u is in V, and if u is related to V and V is in I, then u is in I. Therefore, one can invent ideals as (0,1)-sheaves under higher topos theory.

Ideal, generalisation of even numbers as a ring through decomposition: say you want to decompose a ring like Z itself into unique factorisation, what are the conditions for decomposing Z to sets with prime factor (2), sets with prime factor (3) etc etera while still preserving uniqueness, the idea is to have ra, where r is in the ring, and a is in the ideal required, so start with basic element which is prime p in the ideal I, then consider scalar multiples of these basic elements to be a ring. Further, you want it to have the ring or module structure over the ring. So a ideal is a submodule of the ring, not just a set. For non-commutative ring, scalar multiples by the left give rise to left ideals, and is a left submodule of the full ring R. Therefore, one can invent the notion of an ideal by trying to see what sort of modules are factorised out of a full ring through decomposition.

Ideals, basic case of sheaves: one consider the definition of a sheaf, on only one set, the restriction maps are given by partial order on the elements of a ring A since it exists due to the condition that the ideal I is a submodule of the module of the ring A over itself, uniqueness and existence on overlaps is trivial since containment of opens and the partial order inherent in the ring. Therefore, one can invent ideals by having sheaves valued in {0, 1} where {0, 1} is containment or not.

Kernel, normalisation procedure for a group homomorphism: consider the example of a grassmann algebra and the determinant map as a group homomorphism from the general linear group to the reals under multiplication. The special linear group is of determinant 1, or it is the kernel of the determinant map. Therefore, one could have invented kernels as spaces where group homomorphisms are normalised, in this case the determinant map is normalised to the multiplicative group identity.

Lebesgue integral, failure of bounded functions to have term by term integration, problem is that a function that is not Riemann integrable may be represented as a uniform bounded series of Riemann integrable functions. Replace this with the Lebesgue integral, term by term integration of Lebesgue integral is always valid. How does this relate to measure?

Left adjoint, freest construction for representable: let G be a functor from category D to category C, you want to be able to build some sort of free construction on functor G acting backwards, then forget some more structure for a representable. This mimics the free construction of a group, then imposing additional relations to get whichever group you want. Also, this corresponds to the idea that a group presentation can be determined by its generators, then its relations. This also corresponds to a exact sequence or resolutions.

Left adjoint, preservation of free construction: consider the basic case of a free group, a left adjoint functor is something that acts on all of the alphabets / elements sentences in the free group and still preserves the sentence in the new category. The act of freely forming sentences is a form of colimit, see for example, abab being a + b + a + b (need not be abelian), we have F(a) + F(b) + F(a) + F(b). Therefore, the free functor, which is left adjoint (note, inner Hom product means I can act on FX to Y as a hom, and it is preserved, F is left adjoint. Consider the inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY), we have F is left adjoint to G, G is right adjoint F.

Lie group, preserving teh action functional: ontology is by Noether. The laws of motion work by minimising some action functional. The minima of this action functional remains unchanged, therefore it can be acted on by a group. Therefore one can consider the continuous group of symmetries of the physical system or the action of the Lie group on the configuration space leaving the action functional unchanged. The number of independent quantities give the dimension of the Lie group. One can therefore invent a Lie group by seeing what symmetries does an action functional have. Moral of the story, if something remains unchanged, think symmetry think group theory!

Measure, assignment of a space a real number, design a function m(E) in positive real inclusing infinity such that there is a homomorphism under disjoint sets. This is how you can essential invent the definition of a measure, it is some space that is converted to a real number.

Measure, consideration of non pathological sets, this discussion was by Tao, typical solid body have infinite points of measure zero, linear algebra of points fails when you use Hilbert's paradox of the grand hotel to double the length. Even with finite partitions, you can use very pathological pieces with the axiom of choice to double a sphere using the Banach Tarski paradox. Therefore, one can invent the notion of measurable sets, where a measure can be defined.

Module, generalisation of ideals for a polynomial ring: Kronecker was the one who introduced this older ontology, the idea is to adjoin a root of a polynomial f(x) = 0 in general instead of a prime number to come up with something like a ideal called a module to take modulo module M, so a module can be invented as an ideal but with polynomials, a generalisation of ideals with polynomials and variables.

Module, structure where congruence modulo module M makes sense: consider this lemma or pseudo definition: if all members of submodule A are members of module B, then A is a multiple of B, and B divides A. The set of all elements common to A and B are the least common multiple (intersection), the set of all sums a and b for a in submodule A and b in module B is the greatest common divisor, one can isolate closure under differences (recall modular arithmetic) and absorption of multiplication to make modular arguments. Examples: 5 mod 6 - 2 mod 6 = 3 mod 6, 6 is seen as a module, more generally you can replace 6 with some algebra.

Modules, k-forms or tangent space associated with each open to a vector space via a sheaf: the idea is not new, it is to define a sheaf of modules. We give an example of the sheaf of differentials for a k-algebra, A where k is  a field. We can find the i-th form of a manifold and take alternating powers to give the i-form as A-modules, then take duals to give tangent spaces. Modules can therefore be invented as the ontology necessary to give local properties of rings at each open of a topological space, associating with each open a module over the infinitely differentiable opens of a manifold to form a sheaf of infinitely differentiable modules.

Monads, categorical assignment of distrbution: this ontology was considered by Lawvere, and expanded by Giry, consider the case of a coin determining distribution. A probability tree of n levels can be composed togehher to a single one, so filtrations can be seen as a monoid of endofunctors by considering independent and identical distributions repeatedly on a sample space.

Number field, repeated additions, subtractions, multiplications and divisions generated from a number: the idea is to have a number as a generator, and see what other numbers an you get out of it by adding, subtracting, multiplying and dividing. Turns out these operations are already encoded in the rationals, so you can extend the rationals by this generator, so the definition of a number field is a finite extension of the field of rational numbers. So, one can invent the notion of a number field by seeing how one can freely add, subtract, multiply and divide if you add an additional generator.

Number ring, linear combinations of elements of a number field: the idea is to think of polynomials as linear combinations of a number in a number ring, based on a number field. If you can form the algebraic integer as a root fo a polynomial, it will satisfy the properties of a ring (since the component of a linear combination, or a root of a polynomial equation), therefore one can define a number ring to be the algebraic integers of a number field, where algebraic integers satisfy a polynomial equations, whose scalars multiples (using the analogy from linear algebra) come from a number field.

Open set, a set equal to the set of interior points: Peano was the one to be able to define interior points as interior to a point set A if there is a positive number r such that all those points whose distance from p is less than r belong to A, a point is exterior to A if p is interior to the complement of A, a boundary of a set as a collection of all its boundary points, therefore Peano could've invented open sets as the set of all its interior points.

Open set, complement of a derived set: a derived set is invented by Cantor to be a set containing all of its limit points. This is actually a closed set, so if Cantor took complements we would have defined open sets. One can therefore invent open sets by paying close attention to the complement of closed sets.

Open set, last to come when generalising limit points: start with the idea of a limit point of a set, then a closed set is a set with all its limit points, then take complements to get open sets by thinking about interior and exterior.

Orbit, generalisation of transitivity: say we have a transitive equivalence relation, if x is related to y, and y is related to z, then x is related to z. Instead, we can say a group is transitive on select group actions so it is transitive under group action g, but not f for example. For example, gx = ggy = gggz and these are transitive for this orbit for a left action g. Therefore, one can invent the notion of orbits as the elements in x that exhibit transitivity under left action g of group G. A group is therefore transitive if it has only one orbit. This way, one can invent multi dimensional or case by case transitivity.

Probability, normalised total space admitting integrals: the idea is you can start with measure theory, then use the formula for independent distributions as some sort of homomorphism P(XY) = P(X) P(Y). This condition of independence gives rise to what you can do with probability theory. Note that there is some sort higher order morphism thing going on since X and Y are measurable functions from the sample space to the real line. Then, to reinvent the definition of probability, simply pick P(sample space) = 1. Then you can invent the idea of a probability measure to be the normalised total space.

Random variable, kernel function of integral for probability: start with the idea that a probability is the Lebesgue integral of a probability density function. This comes from intuition of space as a dartboard, take a portion of an area where the total area is 1. Then, we have the case where we need to define a probability density function to be the inverse of a random variable mapping from the reals (our distribution) to the sample space. This is how one reinvents the definition of a random variable to be a measurable function from the sample space to the reals.

Representation, non-commutative rings: idea is to try and reduce algebraic structures into linear algebra, represent elements using linear transformations. Describe it with matrices, and matrix rings are not commutative, so representation theory reduces to the case of studying non-commutative rings. The representations of rings are called modules.

Riemann surfaces, connectivity in topology and root parametrisation: Riemann was studying the Riemann surfaces where f(w,z) = 0 and see how the roots vary as w and z varies, then he defined Riemann surfaces determined by f(w,z) so that w(z) is defined by the equation f(w,z) = 0.

Right adjoint, underlying structure is preserved: consider the case of a limit. Under an action of a right adjoint, we have the inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY), we have F is left adjoint to G, G is right adjoint F. The arrows in the original category C, we have the homomorphisms from X to GY being preserved, G is the right adjoint functor, so we still keep the GY arrows)

Ring, generalising integer arithmetic: idea is to try and prove Fermat's last theorem. Initial idea is to adjoint root of -3 and consider a + b\root(-3). These form a ring, then try to find unique factorisation to show the n = 3 case. In fact, consider numbers of the form a + bw + cw^2, w is root of unity of 1 have unique factorisation. Basically, you consider properties of integers in two separate parts, then see if they combine nicely to still satisfy properties of integers. The idea is x^p + y^p = z^p can be converted to a product of (x + wy) = z^p by factorisation, for p-th root of unity, these rings fail to admit unique factorisation, idea is to generalise integers Z(w_p), fix this my inventing ideals. So, one can invent ring by adjoining roots and stuff.

Ring, generalising number fields, linear algebra and linear combinations through number rings: Hilbert was the first to make this generalisation. Previously, there were number fields. Then, we consider the algebraic integers that satisfy some linear combination (called a polynomial) with coefficients in the number field. These form a number ring. Hilbert generalised this further. But based on history, one can define elements of a ring to be those that can form a certain linear combination akin to a polynomial equation such that the polynomial can be factored as a product. One can invent the notion of a ring as elements that satisfy a linear combination relation.

Ring, linear algebra and factorisation: from the perspective that the a ring consists of elements that can form a linear combination akin to a polynomial, basically a ring are elements that enable linear combinations to be expressed as products of roots via factorisation, so one has a easy way to not just consider some inner product of coefficients and the root itself, but simply just consider the product of roots, for easy composition. The hard part is the unique factorisation. One can invent the notion of a ring to be elements that can be formed into inner products of coefficients and ring elements that can be factored into products of (x - root).

Scheme theoretic intersection, detecting nilpotents and inventing locally ringed space: the idea is to consider unit circle x^2 + y^2 = 1, vertical line x = +- 1, tangency is invisible when we use set theory to determine intersection, the trick is we ask what is the ring/module of polynomial functions on the intersection is, we have that the ring of polynomial functions is the quotient of both defining polynomials (recall we define rings/modules in a way to allow for modulo/quotients to make sense) so R[x,y]/(x^2 - y^2 - 1, x-1) is isomorphic to R[y]/y^2, since it has nontrivial nilpotent (y^2 = 0) it has multiplicity two. This records the value at the intersection point, but also its derivative with respect to the tangent vectors at the intersection point, Therefore, one can invent scheme theoretic intersection by considering how does one define intersections that count two points close together, connected by an infinitely small vector. Algebraic geometry is general since one can use sheaves of modules to work on any open, and modules is basically modular arithmetic with an algebraic variable, so it is very general and powerful.

Sheaves, generalisation of ideals: recall that a definition of an (left) ideal I of a ring A is that for i in the ideal, a in the ring A, aI is in the ideal I, and the ideal I is a left submodule of the module of the ring A over itself. Consider the case where we don't just check if element ai is in the ideal I or not (whether in or not in), but use a fuzzier logic where we can value this in a set called the global sections of the presheaf F, instead of the truth {0, 1}. This motivates sheaves in finding the right definitions to make ideals work and have some sort of unique factorisation of sheaves.

Sigma field/algebra, linear field/algebra of disjoint unions, one can decompose a union with intersection into disjoint sets and their corresponding intersection. Then, one can define a sort of linear algebra on these disjoint sets, and to make these the appropriate algebra, the "bases" will have to be conplements of sets, countable intersections and union so you have a countable bases (if not you have to use some wacky functional analyses).

Sigma field/algebra, right set of properties to define a (probability) measure. The idea is see which cases fail for the definition of a measure, you cannot have uncountable unions of measurable sets. You also want the following things to be measurable, union, symmetric difference, so you want closure under complementation where m(X ++ Y) = m(X union Y) - m(X intersect Y), where ++ is the symmetric difference. Therefore, one invents the notion of a sigma algebra to be closed under countable unions, countable intersections, and arbitrary complementations, where if several sets have a well defined size, their combination or union is okay, and if several events have a well defined probability of of occuring and so should their simultaneous event via intersection, likewise for complementation.

Sigma ring, alternative to sigma field, for a sigma ring, the universe is not measurable, for a sigma field the universe is measurable since it is closed under absolute complementation of an empty point, whereas in a sigma ring, the complementation is relative to non null sets so the universe is not measurable. Therefore, one can invent the notion of a sigma ring by modifying the definition of a sigma field to ensure the universe is not measurable. See Folland's exercise to show that the right properties of sigma fields still hold in defining a measure.

Simple ring, generalisation of a field: the idea is you want to commutative elements of the ring to be a field, and yet it has more elements that is not commutative, so one can invent a simple ring as a extension of a field, with the definition of the center of a simple ring is a field.

Space, infinity groupoids and simplicial sets: this ontology is presented by Voevodsky, it starts with the idea of the fundamental group. The zeroth fundamental group describes pieces of space, the first fundamental group describe pieces of paths / homotopies of space, one can extend this to arbitrary dimensions, and the structure that enables this is an infinity groupoid, a groupoid is a collection of symmetry transformations, so you take the definition of a category and all morphisms are invertible to get the definition of a groupoid. So you take the definition of an infinity category, make all k-morphisms into k-equivalences, and you have an infinity groupoid. Then, you need a practical model to use infinity groupoids as your "ring" of choice to model space, so you use have to invent the homotopy hypothesis, reduce it to the case of simplicial sets, where infinity groupoids are equivalent to the simplicial localisation of topological spaces at their weak homotopy equivalences. Therefore, you can replace all the data you capture as a space invented by considering infinity groupoids as simplical sets, and simplicial objects on open sets of space. Therefore, one can invent spaces as infinity groupoids realised by simplical sets as one possible construction.

Space, modelling via probes or sheaves: the idea is to start with a suitable site (or open sets, open covers, or something more general), now you want to probe into these by putting extra data, the prototypcial example is putting a sheaf of modules onto open covers, now you can pick from many options of sites (open covers, etale covers, ...) with probes (presheaves, copresheaves, or Isbell envelopes), this way one can invent spaces by considering the locally ringed space as the basic example of probes and space, and then allow for all sorts of examples you want.

Space, the formal dual of algebras: say we invent algebras or commutative rings or polynomial rings (modular systems of variables) and we take the formaul dual of the category of commutative rings, we can get a space like object which is the category of finitely generated algebraic affine schemes. Likewise, if we have nice algebras like simplicial objects in the category of commutative rings, then take its formal dual, we get a structured space. The ones that Jacob Lurie is interested in is when you take the formal dual of E-infinity rings, this gives the category of finitely generated algebraic derived affine schemes. Therefore, one can invent the notion of space as the formal dual of algebras. This is till consider pre-geometry, you want some sort of sheafification on it as probes, this will result in the idea of a stack, where you put rings on every open set in space using sheaves, then you can keep track of a lot of information like nilpotence, tangency.

Splitting field, find the smallest field extension given field $K$ so that polynomial $p(x)$ factors linearly in field extension $L[x]$: the idea is one can replace polynomials with a field extension, therefore you want the roots of $p(x)$ over the field $K$ to generate $L$. One can invent the notion of a splitting field, with the field extension $L$ and with the embedding of $K$ into it.

Topological invariant, geometric invariant without measurement: the motivating example of a topological invariant without measureme, for example for a polyhedron the sum of vertices minus the edges plus the faces equals two, and this did nto depend on the size or extent of the polyhedron. Lhuilier showed that you can do better by subtracting 2g, where g is the genus or the number of holes.

Topological space, Konigsberg bridges as independent of distance: graph theory problem that does not care about distance, this freed mathematics from considering only distance.

Topological space, Kuratowski closure axioms

Topological space, finding invariants to have equivalent surfaces: this ontology is due to Jordan, one can invent the idea of studying topological space is to invent topological invariants to check that whether two surfaces are homemorphic or not. Therefore, Jordan was the first to realise that topological invariants and through homeomorphisms can be used to define and classify topological spaces with properties that ensure topological invariants remain unchanged.

Topological space, neighbourhood axioms: start with the idea that topology is about closeness, and closeness can be studied in terms of inclusion and exclusion of sets. Therefore, Hausdorff in his book on set theory defined a topological space via neighbourhoods and topology as the study of closeness through the inclusion and exclusion of neighbourhoods. To reinvent this, first axiom is every point in a set belong to some neighbourhood, supersets of neighbourhoods of points are neighbourhoods, intersections of neighbourhoods of the same point are neighbourhoods. This is the key crucial definition: any neighbourhood N of x includes a neighbourhood M of x such that N is a neighbourhood of each point of M (this links all neighbourhood). A subset is open if it is a neighbourhood of all points in the subset.

Topology, describing the Mobius band: the Mboidus band was discovered by Mobius and motivated the idea of orientation since it is non orientable, therefore Listing invented the word topology by studying connectivity of surfaces. Therefore, one can invent topology by thinking of the Mobius band as an example of something interesting that is beyond measure and it is not orientable.

Topology, from neighbourhoods to open sets: the idea is to define topology based on the two properties of neighbourhoods we want, which is supersets of neighbourhoods are neighbourhoods which means arbitrary unions of open sets are open, and also the three other axioms especially the last one captured by considering finite intersections to belong to the topology (if you allow infinite intersections then closeness can never be reached). This is how one can invent the definition of a topology by starting with the definition of neighbourhoods.

Topology, using limit points and no distance: ontology is due to Riesz, one can invent topological space by discarding all notions of metric, and using only limit points to axiomatise topology.

Type theory, avoiding Russell's paradox: the idea is to avoid Russell's paradox with the set of all sets, and reduce the foundations to a type theory make it impossible to say whether a class was or was not a member of itself. Therefore, one can reinvent type theory based on trying to avoid this paradox.

Yoneda lemma, from the perspective of natural equivalences: the idea is to define natural transformations, or transformations that are twice removed from conjugation. The typical example is Y'' is naturally isomorphic to Y, where ' is taking the dual of the vector space. The Yoneda lemma can be considered as a central definition where Nat(h_a, F) for the natural transformations between hom-sets of h_a and the functor F, is the double dual of the Hom functor Hom(Hom(A, -),F) from a fixed A (covariant means from A to stuff, contravariant reverses the arrow, so you can have Hom(G, Hom(-, A))) to be defined as natural transformations Nat(h^A, G). Then we have F as a functor to be invariant under double conjugation of Hom, so F(A) is naturally isomorphic to Hom(Hom(A, -), F), and we can define this as the natural transformations from morphism h_A to F(A). Note that the original motivation for inventing category theory is say there is an isomorphism between the locally compact ableian group with its twice iterated character group, or there is a natural isomorphism between L -> T^2(L). This is Pontrajagin duality. One can therefore invent the Yoneda lemma as the natural isomorphism between a covariant/contravariant functor of an object, and the double Hom into/out of a fixed object, with the conjugating functor being the Hom-functor.

