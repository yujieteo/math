Scheme theoretic intersection, detecting nilpotents: the idea is to consider unit circle x^2 + y^2 = 1, vertical line x = +- 1, tangency is invisible when we use set theory to determine intersection, the trick is we ask what is the ring/module of polynomial functions on the intersection is, we have that the ring of polynomial functions is the quotient of both defining polynomials (recall we define rings/modules in a way to allow for modulo/quotients to make sense) so R[x,y]/(x^2 - y^2 - 1, x-1) is isomorphic to R[y]/y^2, since it has nontrivial nilpotent (y^2 = 0) it has multiplicity two. This records the value at the intersection point, but also its derivative with respect to the tangent vectors at the intersection point, Therefore, one can invent scheme theoretic intersection by considering how does one define intersections that count two points close together, connected by an infinitely small vector. Algebraic geometry is general since one can use sheaves of modules to work on any open, and modules is basically modular arithmetic with an algebraic variable, so it is very general and powerful.

Modules, k-forms or tangent space associated with each open to a vector space via a sheaf: the idea is not new, it is to define a sheaf of modules. We give an example of the sheaf of differentials for a k-algebra, A where k is  a field. We can find the i-th form of a manifold and take alternating powers to give the i-form as A-modules, then take duals to give tangent spaces. Modules can therefore be invented as the ontology necessary to give local properties of rings at each open of a topological space, associating with each open a module over the infinitely differentiable opens of a manifold to form a sheaf of infinitely differentiable modules.

Global sections functor, representable functor of the affine line: the idea is to think about what functor should an affine line represent as an representing object. One can invent the global sections functor from a topological space X to the global sections of the space X and the sheaf of rings on X through the Yoneda lemma. Some other examples would be Picard schemes representing and defining a Picard functor, Hilbert schemes representing a functor associating a variety with a family of subvarieties.

Adjoining roots, Fermat's last theorem: the idea is to decompose the sum of x^n and y^n into the product of x - root of {-1}^{2i + 1}y, Then one can take the Z[root of -1] to do unique factorisation. So looking at Fermat's last theorem, one can invent the idea of adjoining roots.

Dedekind domains, unique factorisation into prime ideals as Fermat's last theorem: this ontology is due to Dedekind, the idea is to factorise multiplicatively preserving products by representing ring elements by ideal of multiples. One can invent the definition of a Dedekind domain to be either a field or to have the nice condition that every nonzero proper ideal factors into products of prime ideals or to factor into products (category theory applies here).

Module, generalisation of ideals for a polynomial ring: Kronecker was the one who introduced this older ontology, the idea is to adjoin a root of a polynomial f(x) = 0 in general instead of a prime number to come up with something like a ideal called a module to take modulo module M, so a module can be invented as an ideal but with polynomials, a generalisation of ideals with polynomials and variables.

Module, structure where congruence modulo module M makes sense: consider this lemma or pseudo definition: if all members of submodule A are members of module B, then A is a multiple of B, and B divides A. The set of all elements common to A and B are the least common multiple (intersection), the set of all sums a and b for a in submodule A and b in module B is the greatest common divisor, one can isolate closure under differences (recall modular arithmetic) and absorption of multiplication to make modular arguments. Examples: 5 mod 6 - 2 mod 6 = 3 mod 6, 6 is seen as a module, more generally you can replace 6 with some algebra.

Dedekind cuts, real numbers as representing objects of adjoint functors: Dedekind's idea is very categorical. Use poset structure of reals, define real as representing object of the set of rationals less than or equal to the real or a representing object for a representing adjoint functor. This is the Dedekind cut. General lesson, one can define an object representing some functor. Therefore one can indirectly invent Dedekind cuts, or corresponding representing objects of a category through considering real numbers.

Lie group, preserving teh action functional: ontology is by Noether. The laws of motion work by minimising some action functional. The minima of this action functional remains unchanged, therefore it can be acted on by a group. Therefore one can consider the continuous group of symmetries of the physical system or the action of the Lie group on the configuration space leaving the action functional unchanged. The number of independent quantities give the dimension of the Lie group. One can therefore invent a Lie group by seeing what symmetries does an action functional have. Moral of the story, if something remains unchanged, think symmetry think group theory!

Adjunction, Hom inner product: this ontology comes from linear algebra. The idea is simple, the definition of a natural transformation, one definition of an adjunction can come from the definition of linear algebra, consider the adjunction under integral inner product of Int(fx, y) = Int(x, gy), where f and g are linear operators. Likewise, one can define adjunction as an inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY). This way, one can invent the definition of an inner product.

Bonferroni correction, p-hacking correction: too many hypothesis tests means false positive on hypothesis hacking, reduce statistical power.  Works for uncorrelated hypotheses (direct categorical limit of with each study an orthogonal projection) by direct division of number of tests, significance / number of tests. Therefore, one can invent the Bonferroni correction as a way to correct for avoiding p-hacking by having too many hypothesis, under a crude method that the distributions are independent and identically distributed or the chief experiment is a direct limit of these distributions.

Borel-sigma field/algebra, smallest sigma algebra and open sets: say you want to do measure theory of spaces (typically locally Hausdorff) you want to find the smallest basis to do linear algebra with, so one invents the notion of a Borel sigma algebra, the smallest sigma algebra containing the open sets as some sort of base for the linear algebra of points.

Categorification, replacing integers by sets: this adds dimensionality since you can put other objects as set, then reduce it to the case of integers by looking at grading of the set. You can then apply group theoretic techniques or techniques from number theory on the decategorified version of the set. Note that categorification came later.

Complex numbers, example of  a ring: historically, this was done by Gauss to show unique factorisation. So the definition of complex numbers is to enable unique decomposition of integers as a product of prime integers.

Complex numbers, ordered pairs of numbers: once you consider these as ordered pairs of numbers, one can apply topological methods to it since these lie on two different axes, or to apply algebraic geometry. It also lends intuition to 1 and i being orthogonal vectors on the real-imaginary plane, and also motivates the complex axes. Methods from linear algebra in two dimensions, such as defining [a -b; b a] as the matrices that govern complex numbers. In fact it can be written as [cos theta -sin theta; sin theta cos theta] = cos theta * [1 0; 0 1] + sin theta * [0 -1; 1 0] and these matrices represent the real and imaginary unit vectors. Representation theory can then be applied. One can invent complex numbers by looking at the complex plane, and see how to represent it with matrices into one object.

Delta ring, using intersections instead of relative complementation to prevent sets of infinite measure, one can construct a delta ring by instead of consider relative complementation, one uses countable intersections. This prevent sets of infinite measure with the following example where the countable union of intervals of [0,n] forms a delta ring and not a sigma ring since this is not bounded, it is not closed under relative complementation. Therefore, one can invent a delta ring as a stricter condition of a sigma algebra to prevent sets of infinite measure.

Direct quotient, preservations of categorical limits: this is an exact sequence, divisor represents the equivalence classes, the dividend is actually direct limit, direct quotient is a projection. Manipulate the equation to: categorical limit = divisor / projection * number of equivalence classes / projections. If a direct quotient can be done, it means the categorical limit is preserved, and there exists a forgetful functor to cardinality that exists. I first noticed this phenomenon when trying to understand the Bonferroni correction.

Exterior algebra, make geometry arithmetic: recall that Grassmann invented the Grassmannian which have subspaces of a space as points. Start with orthogonal units, adjoin them freely to form linear combinations or sum of a_i e_i. Try to define a volume of it, it fails since you should get negative volume to get multi linearity (in particular, two vectors agree if their oriented volume is 0)

Exterior algebra, quantum mechanics and noncommutative geometry: recall that points are maximal ideals can be studied as algebraic geometry (which is commutative in nature). But, quantum mechanics with points mean observables are commutative ring, so do not use points, use Grassmann numbers or points of the exterior algebras in the first place. Define fermions like this.

Exterior algebra, two sides of algebra: the idea is one can decompose the tensor product of V tensor V into the symmetric square of V, with the symmetric algebra, and the alternative square of V, with the exterior / alternating algebra. So V tensor V is isomorphic to the direct sum of the symmetric square of V and the alternating square of V. They are C[G]-submodules of V tensor V. One can then generalise to define the symmetric and exterior powers as subspaces of the k-th tensor power, not the second tenor powers. No longer decompose as direct sum, but there is a Schur-Weyl duality that gives a formula or this that I do not understand.

Exterior product, multiplicands in exterior: Grassmann came up with the name. Idea is x ^ y ^ z is 0 if x lies in (not the exterior of) the subspace spanned by y and z, or x is in the parallelogram of y and z, so the exterior product vanishes.

Field of set, sigma algebra with finiteness conditions, basically one can invent the notion of a field of sets by stipulating complementation, closure finite unions or equivalently, finite intersections (using De Morgan's laws).

Forgetful functor, right adjoint: recall that a forgetful functor forgets some structure. Consider the case of limits, where you have a big object with orthogonal projection maps. The forgetful functor forget structure and projects down onto the underlying set. Therefore, it is a right adjoint, and right adjoint preserves limits.

Greatest common divisor, preserve information on scalars: consider gcd(ab, ac) = a gcd(b, c) holds because we are taking the greatest of something. This feels very much like adjunctions somehow, but I cannot pin it down.

Group cohomology, characterisation of free group of n generators as wedge sum of circles as example of topological cohomology isomorphism on classifying spaces or delooping BG: the idea is to describe the shape of the tree group Z * Z * Z… generated by n letters can be compared to its interpretation in topology. Recall that to every group G, there is a topological space BG called the classifying space of the group, the idea of the classifying space is a functor, we have pi_1(BG) = G, and pi_k(BG) = 0, for k >= 2, the topological homology is isomorphic to the group cohomology so H^k(BG, Z), topological, is isomorphic to H^k(G, Z). Note that you can replace Z with other stuff. In the case of B(Z * … * Z) for n-letters, you can get it as the wedge sum of n circles, use Van-Kampen’s theorem and compute topological cohomology, so the group cohomology of this is H^k(Z * … Z) is Z for k = 0, Z^n for k = 1, vanishes otherwise. Therefore, one can invent the idea of group cohomology by seeing under classifying spaces, what is the topological cohomology of that space.

Group cohomology, invariants of a group representation: start with the fundamental group of a space. This ontology is from Hurewicz, the idea is to represent a space by its fundamental group since Hopf showed if all higher homotopy groups of a space vanish then cohomology is determined by the fundamental group, then we have pi_1(X) as the fundamental group or the number of pieces of paths. Group cohomology becomes by definition to be H_*(pi_1(X)) of the fundamental group is defined to be cohomology group of the space H_*(X) by abuse of notation, since cohomology can be defined for a group that is made into a module. This is how you can invent group cohomology of a space, when you try to see what groups capture a space fully.

Group cohomology, right derived functor: start by studying a group G by its group representations or G-module, a G -module is an abelian group M (note: different from using a finite set or permutations of set, see consideration of p-group fixed point theorem) together with a group action of G on M, every element of G acting as an automorphism of M, G is written as multiplicatively, M is additive, this is a module. Then consider the submodule of G-invariant elements (or fixed under G, orbit submodule). Now if N is a G-submodule of M, not true that M/N are found unlike the case of abelian group M is a set. The first group cohomology is invented to measure the difference H^1(G, N). Now, form the collection of all G-modules as a category, where morphisms are equivariant group homomorphisms that are group homomorphisms from f(gx) = g(f(x)). Send each module M to group of invariants (or orbits) M^G is a functor from category of G-modules to tech category of Ab of abelian groups, this is left exact, not right exact. Form right derived functor, these are the group cohomology H^n(G, M). This is how one events group cohomology, repeat the same idea for inventing orbits on a finite set, but make the G-module first, then send it to an abelian group of group actions to see how cohomology affects this.

Group modules, group action on abelian groups: a module is a generic version of a vector space, with multiplicative scalars and additive objects. To invent group modules or group representations, pick abelian groups as the additive objects for commutative sums, and group elements as multiplicative scalars. Consider group G acting on a set X, this gives X as a discrete union of orbits. To define G-modules, the idea is to have another abelian group, so we have a G-module is an abelian group M, together with a group action of G on M, M is additive abelian group, G is multiplicative (not abelian) group. Therefore, one can invent the idea of a group module is to at least have a group action on another abelian group.

Group, categories of one object with all isomorphisms: consider that compositions of morphisms are all morphisms, if you add the stipulation of invertibility, and only require to study the symmetries (invertibility) of one object, you get the definition of a group being a groupoid of one object, or alternatively a category of one object with all isomorphisms on that object. One can therefore invent the notion of a group, by knowing what is groupoid, and consider all isomorphisms on a single groupoid object.

Group, generators and relations: consider a finite free group of n-alphabets, this gives the finite symmetric group on n-objects, freely adjoining the alphabet. Since this is a free group, you want to be able compose and take away elements of the free group easily. To get general groups, add additional relators for product of group elements to be the identity of the group to further constrain the definition of a group, these are called the relations. In fact, generators and relations are sufficient to define any finite group, and are enough to define a group. These are called the presentation of the group. One can invent the notion of a group, by seeing how you can make a free group by composing group elements as freely as possible, then you impose some rules to give it more structure and uniqueness.

Group, group actions and representations: groups can be represented by their representations, from the group itself to the general linear group, you want compositions of linear representations be linear representations (or linear transformations), likewise for the trivial representations, and the existence of inverse representations. This also motivates the study of invariants of these matrices, therefore the character of representations is defined by the trace of these linear maps. One can invent the notion of a group, by simply looking at compositions of matrix actions, and see what representations, when multiplied, get back the same trivial representations.

Group, necklaces on a finite set: this ontology gives a very nice fixed point theorem, let group G be a group acting on a finite set X, if cardinality of the finite set X is not divisible by of the the order p of the group G, then there is a fixed point in set X for the group. Proof intuition, is to consider necklaces, we can look at the prime decomposition elements of the finite set X, if there is remainder, these cannot be part of cyclic action of subgroups of the group G, so these are fixed points. Formal statement: Let G be a finite p-group, and let finite set X be acted on by the action of the finite group G, the set of actions being X^G, we note that the cardinality of this set of actions |X^G| is of cardinality X mod p, in particular, if the cardinality of X is not divisible by order p, then there is a fixed point in X^G. Proof, the finite set X is the disjoint union of all orbits acted on by G (these look like distinct necklaces) all looking like G/H, in particular, if we have a case where it is not divisible by order p, then there exists a fixed point.

Group, permutation of roots: Galois invented this ontology, you want to permute the roots of a polynomial equation and compose these permutations. With this ontology, and consider the trivial permutation, and compositions of permutations to be permutations, one arrives at the definition of the group. Also, which permutation when composed with a given permutation gives the trivial permutation? This must be the inverse permutation. Further, one can invent how these permutations of roots work. Given a polynomial equation, you can get the roots of the polynomial equations, that are solutions to algebraic equations. You want to permute the roots of that solve these algebraic equations, to see if you can admit a solution by radicals.

Group, symmetries of object: Cayley invented this ontology, he invented Cayley's theorem, groups are isomorphic to permutation subgroups of the symmetric group. The object remains invariant, so you can compose symmetries to get back the original object, and you can consider the what to compose to the any symmetry transformation to get back the trivial symmetry of doing nothing. This ontology is obtained when one consider that there is a bijection between left action on group elements, to permutations of group elements. To prove Cayley's theorem, consider the mapping a group element x to the left action on the group element gx, since the composition of group elements is closed by preserving the symmetries of an object, this must be a bijection on the set of group elements. Since this is a bijection on the set of group elements, the set of group elements is in bijection to the permutation of the group elements, and is hence isomorphic to a permutation subgroup of the symmetric group. The symmetric group is also a automorphism group on sets. One can invent the notion of a group by considering all symmetries of an object.

Ideal, generalisation of even numbers as a ring through decomposition: say you want to decompose a ring like Z itself into unique factorisation, what are the conditions for decomposing Z to sets with prime factor (2), sets with prime factor (3) etc etera while still preserving uniqueness, the idea is to have ra, where r is in the ring, and a is in the ideal required, so start with basic element which is prime p in the ideal I, then consider scalar multiples of these basic elements to be a ring. Further, you want it to have the ring or module structure over the ring. So a ideal is a submodule of the ring, not just a set. For non-commutative ring, scalar multiples by the left give rise to left ideals, and is a left submodule of the full ring R. Therefore, one can invent the notion of an ideal by trying to see what sort of modules are factorised out of a full ring through decomposition.

Lebesgue integral, failure of bounded functions to have term by term integration, problem is that a function that is not Riemann integrable may be represented as a uniform bounded series of Riemann integrable functions. Replace this with the Lebesgue integral, term by term integration of Lebesgue integral is always valid. How does this relate to measure?

Left adjoint, freest construction for representable: let G be a functor from category D to category C, you want to be able to build some sort of free construction on functor G acting backwards, then forget some more structure for a representable. This mimics the free construction of a group, then imposing additional relations to get whichever group you want. Also, this corresponds to the idea that a group presentation can be determined by its generators, then its relations. This also corresponds to a exact sequence or resolutions.

Left adjoint, preservation of free construction: consider the basic case of a free group, a left adjoint functor is something that acts on all of the alphabets / elements sentences in the free group and still preserves the sentence in the new category. The act of freely forming sentences is a form of colimit, see for example, abab being a + b + a + b (need not be abelian), we have F(a) + F(b) + F(a) + F(b). Therefore, the free functor, which is left adjoint (note, inner Hom product means I can act on FX to Y as a hom, and it is preserved, F is left adjoint. Consider the inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY), we have F is left adjoint to G, G is right adjoint F.

Measure, assignment of a space a real number, design a function m(E) in positive real inclusing infinity such that there is a homomorphism under disjoint sets. This is how you can essential invent the definition of a measure, it is some space that is converted to a real number.

Measure, consideration of non pathological sets, this discussion was by Tao, typical solid body have infinite points of measure zero, linear algebra of points fails when you use Hilbert's paradox of the grand hotel to double the length. Even with finite partitions, you can use very pathological pieces with the axiom of choice to double a sphere using the Banach Tarski paradox. Therefore, one can invent the notion of measurable sets, where a measure can be defined.

Monads, categorical assignment of distrbution: this ontology was considered by Lawvere, and expanded by Giry, consider the case of a coin determining distribution. A probability tree of n levels can be composed togehher to a single one, so filtrations can be seen as a monoid of endofunctors by considering independent and identical distributions repeatedly on a sample space.

Number field, repeated additions, subtractions, multiplications and divisions generated from a number: the idea is to have a number as a generator, and see what other numbers an you get out of it by adding, subtracting, multiplying and dividing. Turns out these operations are already encoded in the rationals, so you can extend the rationals by this generator, so the definition of a number field is a finite extension of the field of rational numbers. So, one can invent the notion of a number field by seeing how one can freely add, subtract, multiply and divide if you add an additional generator.

Number ring, linear combinations of elements of a number field: the idea is to think of polynomials as linear combinations of a number in a number ring, based on a number field. If you can form the algebraic integer as a root fo a polynomial, it will satisfy the properties of a ring (since the component of a linear combination, or a root of a polynomial equation), therefore one can define a number ring to be the algebraic integers of a number field, where algebraic integers satisfy a polynomial equations, whose scalars multiples (using the analogy from linear algebra) come from a number field.

Orbit, generalisation of transitivity: say we have a transitive equivalence relation, if x is related to y, and y is related to z, then x is related to z. Instead, we can say a group is transitive on select group actions so it is transitive under group action g, but not f for example. For example, gx = ggy = gggz and these are transitive for this orbit for a left action g. Therefore, one can invent the notion of orbits as the elements in x that exhibit transitivity under left action g of group G. A group is therefore transitive if it has only one orbit. This way, one can invent multi dimensional or case by case transitivity.

Probability, normalised total space admitting integrals: the idea is you can start with measure theory, then use the formula for independent distributions as some sort of homomorphism P(XY) = P(X) P(Y). This condition of independence gives rise to what you can do with probability theory. Note that there is some sort higher order morphism thing going on since X and Y are measurable functions from the sample space to the real line. Then, to reinvent the definition of probability, simply pick P(sample space) = 1. Then you can invent the idea of a probability measure to be the normalised total space.

Random variable, kernel function of integral for probability: start with the idea that a probability is the Lebesgue integral of a probability density function. This comes from intuition of space as a dartboard, take a portion of an area where the total area is 1. Then, we have the case where we need to define a probability density function to be the inverse of a random variable mapping from the reals (our distribution) to the sample space. This is how one reinvents the definition of a random variable to be a measurable function from the sample space to the reals.

Representation, non-commutative rings: idea is to try and reduce algebraic structures into linear algebra, represent elements using linear transformations. Describe it with matrices, and matrix rings are not commutative, so representation theory reduces to the case of studying non-commutative rings. The representations of rings are called modules.

Right adjoint, underlying structure is preserved: consider the case of a limit. Under an action of a right adjoint, we have the inner product of Hom_D(FX, Y) to be naturally isomorphic (under action of double Hom) to Hom_C(X, GY), we have F is left adjoint to G, G is right adjoint F. The arrows in the original category C, we have the homomorphisms from X to GY being preserved, G is the right adjoint functor, so we still keep the GY arrows)

Ring, generalising integer arithmetic: idea is to try and prove Fermat's last theorem. Initial idea is to adjoint root of -3 and consider a + b\root(-3). These form a ring, then try to find unique factorisation to show the n = 3 case. In fact, consider numbers of the form a + bw + cw^2, w is root of unity of 1 have unique factorisation. Basically, you consider properties of integers in two separate parts, then see if they combine nicely to still satisfy properties of integers. The idea is x^p + y^p = z^p can be converted to a product of (x + wy) = z^p by factorisation, for p-th root of unity, these rings fail to admit unique factorisation, idea is to generalise integers Z(w_p), fix this my inventing ideals. So, one can invent ring by adjoining roots and stuff.

Ring, generalising number fields, linear algebra and linear combinations through number rings: Hilbert was the first to make this generalisation. Previously, there were number fields. Then, we consider the algebraic integers that satisfy some linear combination (called a polynomial) with coefficients in the number field. These form a number ring. Hilbert generalised this further. But based on history, one can define elements of a ring to be those that can form a certain linear combination akin to a polynomial equation such that the polynomial can be factored as a product. One can invent the notion of a ring as elements that satisfy a linear combination relation.

Ring, linear algebra and factorisation: from the perspective that the a ring consists of elements that can form a linear combination akin to a polynomial, basically a ring are elements that enable linear combinations to be expressed as products of roots via factorisation, so one has a easy way to not just consider some inner product of coefficients and the root itself, but simply just consider the product of roots, for easy composition. The hard part is the unique factorisation. One can invent the notion of a ring to be elements that can be formed into inner products of coefficients and ring elements that can be factored into products of (x - root).

Sigma field/algebra, linear field/algebra of disjoint unions, one can decompose a union with intersection into disjoint sets and their corresponding intersection. Then, one can define a sort of linear algebra on these disjoint sets, and to make these the appropriate algebra, the "bases" will have to be conplements of sets, countable intersections and union so you have a countable bases (if not you have to use some wacky functional analyses).

Sigma field/algebra, right set of properties to define a (probability) measure. The idea is see which cases fail for the definition of a measure, you cannot have uncountable unions of measurable sets. You also want the following things to be measurable, union, symmetric difference, so you want closure under complementation where m(X ++ Y) = m(X union Y) - m(X intersect Y), where ++ is the symmetric difference. Therefore, one invents the notion of a sigma algebra to be closed under countable unions, countable intersections, and arbitrary complementations, where if several sets have a well defined size, their combination or union is okay, and if several events have a well defined probability of of occuring and so should their simultaneous event via intersection, likewise for complementation.

Sigma ring, alternative to sigma field, for a sigma ring, the universe is not measurable, for a sigma field the universe is measurable since it is closed under absolute complementation of an empty point, whereas in a sigma ring, the complementation is relative to non null sets so the universe is not measurable. Therefore, one can invent the notion of a sigma ring by modifying the definition of a sigma field to ensure the universe is not measurable. See Folland's exercise to show that the right properties of sigma fields still hold in defining a measure.

Yoneda lemma, from the perspective of natural equivalences: the idea is to define natural transformations, or transformations that are twice removed from conjugation. The typical example is Y'' is naturally isomorphic to Y, where ' is taking the dual of the vector space. The Yoneda lemma can be considered as a central definition where Nat(h_a, F) for the natural transformations between hom-sets of h_a and the functor F, is the double dual of the Hom functor Hom(Hom(A, -),F) from a fixed A (covariant means from A to stuff, contravariant reverses the arrow, so you can have Hom(G, Hom(-, A))) to be defined as natural transformations Nat(h^A, G). Then we have F as a functor to be invariant under double conjugation of Hom, so F(A) is naturally isomorphic to Hom(Hom(A, -), F), and we can define this as the natural transformations from morphism h_A to F(A). Note that the original motivation for inventing category theory is say there is an isomorphism between the locally compact ableian group with its twice iterated character group, or there is a natural isomorphism between L -> T^2(L). This is Pontrajagin duality. One can therefore invent the Yoneda lemma as the natural isomorphism between a covariant/contravariant functor of an object, and the double Hom into/out of a fixed object, with the conjugating functor being the Hom-functor.